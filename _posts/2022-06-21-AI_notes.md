---
layout: distill
title:  AI_CLassNotes_Week2
date: 2022-06-21 21:01:00
toc: true

authors:
  - name: Elias Lee
    affiliations:
      name: KAIST, IDB



description: This are Class Notes From AI504 Class
tags: KAIST
categories:
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

Disclaimer:
This Page is a classNote from the Class AI504 week 2 taught by prof. Edwardchoi at KAIST
<br>
<br>


# Week2: Basic Machine Learning

# What is machine learning?

## AI

AI: artificial intelligence is everything in a machine/computers that mimic human intelligence (does not necessarily needs the “Deep learning” component)
**ex) chess playing program**

## Machine Learning

ML: Statistical method to teach machine to learn from data and execute specific task
**Ex) Spam Filtering**

## Deep Learning

DL: Subdomain of Machine Learning, Applies neural network and massive data to execute machine learning
Ex) ALpha Go

{% include figure.html path="assets/markdown_assets/AI_notes/screenshot.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

# Why Deep Learning?

- We need less feature engineering
- You dont need to define what feature are doglike or catlike

# Machine Learning Categories

## Supervised Learning

Learn a function that maps an input (x) to an output (y)

Ex) Image classification, translation, Image captioning

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

## Unsupervised Learning

Learn a distribution/manifold function of data (x). Theres no label **y**

Ex) Clustering, Topic Model, Low-rank matrix factorization, Generative models

## Reinforcement Learning

There is an environment (E) and a set of action (A), learn a function that maximizes the long-term rewards (R)

Ex) alpha-Go, any game

The lines between Supervised Learning and Unsupervised Learning are a bit blurry. (Generative models and self-supervised learning)

# Optimization

In all of the three categories (SL, UL, RL) you need to train your model (a.k.a learn a function)

## How to train a model $f(x; \theta)$

1. You need a goal (objective)
2. We need a function to achieve goal (objective function)
3. Define the loss function
$loss (y,y')$
4. Find the $\theta$  that minimizes (optimize)
$loss(y, f(x;\theta))$

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled1.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

### Gradient Descent

- Gradient Descent does not guarantee that you are in the global minimum
    - You need to iterate over a number of epochs to increase the chances of achieving a global minimum

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled2.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

### Stochastic Gradient Descent

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled3.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

### How do you know when to stop SGD?

- Using the value of a loss function is not very intuitive
    - what does $10^{-3}$ or $10^{-4}$ means?
- For that we have popular evaluation metrics
    - Accuracy (Muti classification)
    - AUROC (Binary Classification)
    - Precision & Recall
    - BLEU Score (Machine Translation)
    - Perplexity (Language modeling or Text generation)
    - FID Score (Image data)

# Train & Validation & Test

- Training Set
    - Used to train the model

- Validation Set
    - Used this to evaluate model

- Test Set
    - Needs to remain unseen
    - Cannot use this set during training set

## N-fold Cross Validation

- Testing the model’s performance in diverse train/validation/test splits
    - In the example below we divide the data into 8 subsets and rotate the training-validation data and test data in each iteration to verify that the results were not out of luck.
- Not often used in modern deep learning

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled4.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

## Overfitting & Underfitting

- Basically you have to leverage between data complexity vs model capacity
    
    {% include figure.html path="assets/markdown_assets/AI_notes/Untitled5.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}
    
- Doing to well on training data does **not always transmit to good  test performance**

## The Curse of Dimensionality

- When a model is **underfitting** you can add
    - More features to find a better fit.
        - Altough you increase linear in the number of the feature.
            - That increases exponentially the number of data
                  <img src="https://raw.githubusercontent.com/elias-lee/blog2/gh-pages/assets/markdown_assets/AI_notes/Untitled6.png" width="700"/>
                <!-- ![The Curse of Dimensionality](https://raw.githubusercontent.com/elias-lee/blog2/gh-pages/assets/markdown_assets/AI_notes/Untitled6.png) -->
                
    - Add complexity to the model
    - Train longer

- When a model is over fitting you can
    - Restrict the freedom of your model
    - Downsize the hypothesis space
        <img src="https://raw.githubusercontent.com/elias-lee/blog2/gh-pages/assets/markdown_assets/AI_notes/Untitled7.png" width="700"/>
    - You can use L1, L2, etc.. (anything that restrict the freedom)

# Popular Classifier of ML

## Logstic Regression

- Probability → Odds → Log of Odds (Logit)

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled8.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

## Support Vector Machine

- Very famous before Deep Learning
- Maximize the margin between two cases
- Trained via constrained optimization (KKT condition)
    - Today you use gradient descent with hinge loss

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled9.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

## Decision Tree

- Build a tree based on features
- Trained via the CART algorithm

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled10.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

## Ensembles

- Use multiple classifiers to improve performance

- ***Bagging***
    - Train multiple classifiers on different subset of data
    - Train multiple classifiers on different subset of features

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled11.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

- ***Boosting***
    - (K+1)th classifier tries to correct the k-th classifiers errors.
    - A kind of a chain of classifiers

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled12.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

# Popular Clustering

## K-means

- Updates membership of each sample using the closest centroid
- Update the centroid value using all the member samples
- Repeat

{% include figure.html path="assets/markdown_assets/AI_notes/Untitled13.png" title="AI_ML_DL" class="img-fluid rounded z-depth-1" %}

# Resource

[Programming for AI (AI504, Fall 2020), Class 2: Basic Machine Learning](https://www.youtube.com/watch?v=WMtgssrJteA)

[Programming for AI (AI504, Fall 2020), Practice 2: Basic Machine Learning](https://www.youtube.com/watch?v=Rt3TgEemVYI)